<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>用PaddlePaddle学习NLP</title>
      <link href="/2023/08/10/paddlenlp%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/08/10/paddlenlp%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1>用PaddlePaddle学习NLP</h1><h2 id="一-自监督词表示">一.自监督词表示</h2><p>传统的文本处理方式,比如One-hot独热编码虽然能够很好的将词语转换成向量，但是经过独热编码处理后的<strong>词表特征稀疏，词之间相互独立，没有顺序关系，也不能表证词与词之间的关系</strong>。为了弥补这样的短板，我们使用Word2Vec词向量模型和embadding编码。<img src="https://www.ryanieeee.cc/img/paddlenlp-learning/1.PNG" alt="pic-1"><br><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/2.PNG" alt="pic-2"><br><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/3.PNG" alt="pic-3"><br>由此我们提出了两种不同的算法：一种叫做Skip-gram,另一种叫做CBoW</p><h3 id="Skip-gram">Skip-gram</h3><p><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/4.PNG" alt="pic-4"><br>如上图所示，这个算法就是给你一个中间词，让你去预测上下文都是什么简单说一下运算流程：首先我们在一段句子里选定一个中间词Wt，对它进行one-hot编码，然后将它与embadding矩阵（就是W矩阵）相乘，就得到了它对应的词向量，然后我们再将它与分类矩阵W’相乘，最后经过softmax，就可以得到词表中每一个词出现的概率，然后我们这个算法的目标就是最大化上下文窗口的概率<img src="https://www.ryanieeee.cc/img/paddlenlp-learning/5.PNG" alt="pic-5"><br>我们假设蓝色框表示中间词，白框表示上下文的窗口，训练样本对（中间词，上下文target）从直觉上来看，<strong>越是相关的单词出现在同一个上下文窗口也就是组成训练样本对的概率就越大</strong>，这样一来我们就可以学习词之间的关系</p><h3 id="CBoW">CBoW</h3><p><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/6.png" alt="pic-6"><br>它的运算过程和Skip-gram是相反的，中间过程就不细说，最后的目的是要让选定中间词的概率最大化那么两种方式孰优孰劣呢？<br><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/7.png" alt="pic-7"></p><h2 id="二-句子编码神经网络">二.句子编码神经网络</h2><p><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/8.png" alt="pic-8"><br>最开始我们使用自回归语言模型进行句子补全任务，后来我们对这种模型（即潜变量自回归模型）进行改进，于是就出现了RNN<br><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/9.png" alt="pic-9"><br><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/10.png" alt="pic-10"><br>如上图所示，RNN还是存在一定的缺点，比如无法在time step维度上并行，而且长序列处理起来速度很慢，于是我们在传统rnn上加入了self-attention自注意力机制，从而改善了传统rnn</p><h3 id="transformer">transformer</h3><p>目前来讲我们的nlp模型基本上都是用transformer来做的<img src="https://www.ryanieeee.cc/img/paddlenlp-learning/11.png" alt="pic-11"><br>那么在transformer里self-attention又是很重要的一部分，那么在这里我就先简单讲一下self-attention的基本工作逻辑：</p><h4 id="self-attention">self-attention</h4><p>首先说一下注意力机制里面的三个名称：<br>Q(query):可以认为是一个问题，或者说是一个关注点<br>K(key)：键，通过计算query与key之间的相似度来确定对齐程度<br>V(value)：与key相对应的真实值<br><em><strong>我们可以通过计算&quot;query&quot;和&quot;key&quot;之间的相似度来确定对齐程度，然后使用这些对齐程度权重来加权求和&quot;values&quot;，从而得到注意力分布。“query&quot;可以被认为是一个问题或者一个关注点，用于确定哪些&quot;key&quot;与之相关，并据此选择相应的&quot;values”。</strong></em><br>我们假设有一个句子有四个单词，每个单词都映射到三个不同的维度：Q,K,V<br><strong>阶段1</strong>首先，我们让一个单词分别与四个单词的K作内积，得到了四个score<br><strong>阶段2</strong>： 之后呢我们对这四个score作softmax归一化，然后我们就得到了0~1的概率分布，这个分布就是注意力的权重<br><strong>阶段</strong>3： 我们用这个权重矩阵和V矩阵作内积，这样我们就得到了新的这四个单词（句子）的表示这就是self-attention的基本逻辑，我们的transformer中使用的是Mutil-Head Attention机制<img src="https://www.ryanieeee.cc/img/paddlenlp-learning/12.png" alt="pic-12"><br>也就是作多个参数的self-attention，并且把它们拼接在一起，这样做的好处是可以让模型能够注意到文本不同角度的理解。但是和rnn不同，self-attention不会注意到词之间的顺序问题<img src="https://www.ryanieeee.cc/img/paddlenlp-learning/13.png" alt="pic-13"><br>就比如这个句子，里面有3个can，如果用self-attention的话就无法正确理解句子了，为此，我们添加了位置编码（Positional Embadding），这样一来就可以解决时序问题了。那么随着模型的深度不断增加，很容易出现<strong>梯度消失</strong>和<strong>数据分布变化</strong>，那么为了解决这两个问题我们引入了以下解决方式:</p><h4 id="Residual-connections残差连接与Layernormalization层归一化">Residual connections残差连接与Layernormalization层归一化</h4><p><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/14.png" alt="pic-14"><br>那么在这里我就不展开来说了（汗）</p><h4 id="到此">到此,</h4><p>我们已经把前预训练时代的几个代表性模型学习了一下，接着我们就要进入到预训练时代的学习了</p><h2 id="三-预训练语言模型">三.预训练语言模型</h2><h3 id="发展总览图">发展总览图</h3><p><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/15.png" alt="pic-15"><br>上面就是预训练模型的发展图，那么在这里也没啥好介绍的，反正就是一代更比一代强hhh<br>下面我将通过一个开发实例，具体说一下预训练模型的应用</p><h3 id="基于ERNIE-Gram实现语义匹配">基于ERNIE-Gram实现语义匹配</h3><h4 id="导入各种依赖和数据集">导入各种依赖和数据集</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">import paddle</span><br><span class="line">import paddle.nn.functional as F</span><br><span class="line">from paddlenlp.datasets import load_dataset</span><br><span class="line">import paddlenlp</span><br><span class="line"></span><br><span class="line"># 一键加载 Lcqmc 的训练集、验证集</span><br><span class="line">train_ds, dev_ds = load_dataset(&quot;lcqmc&quot;, splits=[&quot;train&quot;, &quot;dev&quot;])</span><br><span class="line"></span><br><span class="line"># 输出训练集的前 10 条样本</span><br><span class="line">for idx, example in enumerate(train_ds):</span><br><span class="line">    if idx &lt;= 10:</span><br><span class="line">        print(example)</span><br></pre></td></tr></table></figure><p><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/16.png" alt="pic-16"><br>label表示相关度：1表示相关，0表示不相关这里说明一下，我这儿使用的数据集是baidu公司提供的千言数据集，是人工制作的，所以使用效果较好</p><h4 id="文本预处理">文本预处理</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 因为是基于预训练模型 ERNIE-Gram 来进行，所以需要首先加载 ERNIE-Gram 的 tokenizer，</span><br><span class="line"># 后续样本转换函数基于 tokenizer 对文本进行切分</span><br><span class="line">tokenizer = paddlenlp.transformers.ErnieGramTokenizer.from_pretrained(&#x27;ernie-gram-zh&#x27;)</span><br></pre></td></tr></table></figure><p>定义样本转换函数，把文字变为token-id</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def convert_example(example, tokenizer, max_seq_length=512, is_test=False):</span><br><span class="line"></span><br><span class="line">    query, title = example[&quot;query&quot;], example[&quot;title&quot;]</span><br><span class="line"></span><br><span class="line">    encoded_inputs = tokenizer(</span><br><span class="line">        text=query, text_pair=title, max_seq_len=max_seq_length)</span><br><span class="line"></span><br><span class="line">    input_ids = encoded_inputs[&quot;input_ids&quot;]</span><br><span class="line">    token_type_ids = encoded_inputs[&quot;token_type_ids&quot;]</span><br><span class="line"></span><br><span class="line">    if not is_test:</span><br><span class="line">        label = np.array([example[&quot;label&quot;]], dtype=&quot;int64&quot;)</span><br><span class="line">        return input_ids, token_type_ids, label</span><br><span class="line">    # 在预测或者评估阶段，不返回 label 字段</span><br><span class="line">    else:</span><br><span class="line">        return input_ids, token_type_ids</span><br></pre></td></tr></table></figure><p><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/17.png" alt="pic-17"><br>从上图可以每一个具有三个位置编码:第一个编码就是token编码，第二个编码是来区分属于这个token第一个句子还是第二个句子，第三个编码是给所有的token排序然后每一个句子的开头都会被插入一个特殊符号：CLS，两个句子中间都会插入SEP特殊符号举个例子：‘喜欢打篮球的男生喜欢什么样的女生’, ‘title’: ‘爱打篮球的男生喜欢什么样的女生’, ‘label’: 1}</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_ids, token_type_ids, label = convert_example(train_ds[0], tokenizer)</span><br><span class="line">print(input_ids) #1表示CLS，2表示SEP</span><br></pre></td></tr></table></figure><p>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(label) #0表示这两句话不匹配，1表示匹配</span><br></pre></td></tr></table></figure><p>[1]</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 为了后续方便使用，我们使用python偏函数（partial）给 convert_example 赋予一些默认参数</span><br><span class="line">from functools import partial</span><br><span class="line"></span><br><span class="line"># 训练集和验证集的样本转换函数</span><br><span class="line">trans_func = partial(</span><br><span class="line">    convert_example,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    max_seq_length=512)</span><br></pre></td></tr></table></figure><p>对数据进行padding并组合成batch</p><p><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/18.png" alt="pic-18"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from paddlenlp.data import Stack, Pad, Tuple</span><br><span class="line"># 我们的训练数据会返回 input_ids, token_type_ids, labels 3 个字段</span><br><span class="line"># 因此针对这 3 个字段需要分别定义 3 个组 batch 操作</span><br><span class="line">batchify_fn = lambda samples, fn=Tuple(</span><br><span class="line">    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids</span><br><span class="line">    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids</span><br><span class="line">    Stack(dtype=&quot;int64&quot;)  # label</span><br><span class="line">): [data for data in fn(samples)]</span><br></pre></td></tr></table></figure><p>定义Dataloader</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 定义分布式 Sampler: 自动对训练数据进行切分，支持多卡并行训练</span><br><span class="line">batch_sampler = paddle.io.DistributedBatchSampler(train_ds, batch_size=32, shuffle=True)</span><br><span class="line"># 基于 train_ds 定义 train_data_loader</span><br><span class="line"># 因为我们使用了分布式的 DistributedBatchSampler, train_data_loader 会自动对训练数据进行切分</span><br><span class="line">train_data_loader = paddle.io.DataLoader(</span><br><span class="line">        dataset=train_ds.map(trans_func),</span><br><span class="line">        batch_sampler=batch_sampler,</span><br><span class="line">        collate_fn=batchify_fn,</span><br><span class="line">        return_list=True)</span><br><span class="line"># 针对验证集数据加载，我们使用单卡进行评估，所以采用 paddle.io.BatchSampler 即可</span><br><span class="line"># 定义 dev_data_loader</span><br><span class="line">batch_sampler = paddle.io.BatchSampler(dev_ds, batch_size=32, shuffle=False)</span><br><span class="line">dev_data_loader = paddle.io.DataLoader(</span><br><span class="line">        dataset=dev_ds.map(trans_func),</span><br><span class="line">        batch_sampler=batch_sampler,</span><br><span class="line">        collate_fn=batchify_fn,</span><br><span class="line">        return_list=True)</span><br></pre></td></tr></table></figure><h4 id="模型搭建">模型搭建</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import paddle.nn as nn</span><br><span class="line"></span><br><span class="line"># 我们基于 ERNIE-Gram 模型结构搭建 Point-wise 语义匹配网络</span><br><span class="line"># 所以此处先定义 ERNIE-Gram 的 pretrained_model</span><br><span class="line">pretrained_model = paddlenlp.transformers.ErnieGramModel.from_pretrained(&#x27;ernie-gram-zh&#x27;)</span><br><span class="line">#pretrained_model = paddlenlp.transformers.ErnieModel.from_pretrained(&#x27;ernie-1.0&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class PointwiseMatching(nn.Layer):</span><br><span class="line">   </span><br><span class="line">    # 此处的 pretained_model 在本例中会被 ERNIE-Gram 预训练模型初始化</span><br><span class="line">    def __init__(self, pretrained_model, dropout=None):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.ptm = pretrained_model</span><br><span class="line">        self.dropout = nn.Dropout(dropout if dropout is not None else 0.1)</span><br><span class="line"></span><br><span class="line">        # 语义匹配任务: 相似、不相似 2 分类任务</span><br><span class="line">        self.classifier = nn.Linear(self.ptm.config[&quot;hidden_size&quot;], 2)</span><br><span class="line"></span><br><span class="line">    def forward(self,</span><br><span class="line">                input_ids,</span><br><span class="line">                token_type_ids=None,</span><br><span class="line">                position_ids=None,</span><br><span class="line">                attention_mask=None):</span><br><span class="line"></span><br><span class="line">        # 此处的 Input_ids 由两条文本的 token ids 拼接而成</span><br><span class="line">        # token_type_ids 表示两段文本的类型编码</span><br><span class="line">        # 返回的 cls_embedding 就表示这两段文本经过模型的计算之后而得到的语义表示向量</span><br><span class="line">        _, cls_embedding = self.ptm(input_ids, token_type_ids, position_ids,</span><br><span class="line">                                    attention_mask)</span><br><span class="line"></span><br><span class="line">        cls_embedding = self.dropout(cls_embedding)</span><br><span class="line"></span><br><span class="line">        # 基于文本对的语义表示向量进行 2 分类任务</span><br><span class="line">        logits = self.classifier(cls_embedding)</span><br><span class="line">        probs = F.softmax(logits)</span><br><span class="line"></span><br><span class="line">        return probs</span><br><span class="line"></span><br><span class="line"># 定义 Point-wise 语义匹配网络</span><br><span class="line">model = PointwiseMatching(pretrained_model)</span><br></pre></td></tr></table></figure><h4 id="模型训练">模型训练</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from paddlenlp.transformers import LinearDecayWithWarmup</span><br><span class="line">epochs = 3</span><br><span class="line">num_training_steps = len(train_data_loader) * epochs</span><br><span class="line"># 定义 learning_rate_scheduler，负责在训练过程中对 lr 进行调度</span><br><span class="line">lr_scheduler = LinearDecayWithWarmup(5E-5, num_training_steps, 0.0)</span><br><span class="line"># Generate parameter names needed to perform weight decay. 权重衰减</span><br><span class="line"># All bias and LayerNorm parameters are excluded.</span><br><span class="line">decay_params = [</span><br><span class="line">    p.name for n, p in model.named_parameters()</span><br><span class="line">    if not any(nd in n for nd in [&quot;bias&quot;, &quot;norm&quot;])</span><br><span class="line">]</span><br><span class="line"># 定义 Optimizer</span><br><span class="line">optimizer = paddle.optimizer.AdamW(</span><br><span class="line">    learning_rate=lr_scheduler,</span><br><span class="line">    parameters=model.parameters(),</span><br><span class="line">    weight_decay=0.0,</span><br><span class="line">    apply_decay_param_fun=lambda x: x in decay_params)</span><br><span class="line"></span><br><span class="line"># 采用交叉熵 损失函数</span><br><span class="line">criterion = paddle.nn.loss.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"># 评估的时候采用准确率指标</span><br><span class="line">metric = paddle.metric.Accuracy()</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 因为训练过程中同时要在验证集进行模型评估，因此我们先定义评估函数</span><br><span class="line">@paddle.no_grad()</span><br><span class="line">def evaluate(model, criterion, metric, data_loader, phase=&quot;dev&quot;):</span><br><span class="line">    model.eval()</span><br><span class="line">    metric.reset()</span><br><span class="line">    losses = []</span><br><span class="line">    for batch in data_loader:</span><br><span class="line">        input_ids, token_type_ids, labels = batch</span><br><span class="line">        probs = model(input_ids=input_ids, token_type_ids=token_type_ids)</span><br><span class="line">        loss = criterion(probs, labels)</span><br><span class="line">        losses.append(loss.numpy())</span><br><span class="line">        correct = metric.compute(probs, labels)</span><br><span class="line">        metric.update(correct)</span><br><span class="line">        accu = metric.accumulate()</span><br><span class="line">    print(&quot;eval &#123;&#125; loss: &#123;:.5&#125;, accu: &#123;:.5&#125;&quot;.format(phase,</span><br><span class="line">                                                    np.mean(losses), accu))</span><br><span class="line">    model.train()</span><br><span class="line">    metric.reset()</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">global_step = 0</span><br><span class="line">tic_train = time.time()</span><br><span class="line"></span><br><span class="line">for epoch in range(1, epochs + 1):</span><br><span class="line">    for step, batch in enumerate(train_data_loader, start=1):</span><br><span class="line"></span><br><span class="line">        input_ids, token_type_ids, labels = batch</span><br><span class="line">        probs = model(input_ids=input_ids, token_type_ids=token_type_ids)</span><br><span class="line">        loss = criterion(probs, labels)</span><br><span class="line">        correct = metric.compute(probs, labels)</span><br><span class="line">        metric.update(correct)</span><br><span class="line">        acc = metric.accumulate()</span><br><span class="line"></span><br><span class="line">        global_step += 1</span><br><span class="line">        </span><br><span class="line">        # 每间隔 10 step 输出训练指标</span><br><span class="line">        if global_step % 10 == 0:</span><br><span class="line">            print(</span><br><span class="line">                &quot;global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s&quot;</span><br><span class="line">                % (global_step, epoch, step, loss, acc,</span><br><span class="line">                    10 / (time.time() - tic_train)))</span><br><span class="line">            tic_train = time.time()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        optimizer.clear_grad()</span><br><span class="line"></span><br><span class="line">        # 每间隔 100 step 在验证集和测试集上进行评估</span><br><span class="line">        if global_step % 100 == 0:</span><br><span class="line">            evaluate(model, criterion, metric, dev_data_loader, &quot;dev&quot;)</span><br><span class="line">            </span><br><span class="line"># 训练结束后，存储模型参数</span><br><span class="line">save_dir = os.path.join(&quot;checkpoint&quot;, &quot;model_%d&quot; % global_step)</span><br><span class="line">os.makedirs(save_dir)</span><br><span class="line"></span><br><span class="line">save_param_path = os.path.join(save_dir, &#x27;model_state.pdparams&#x27;)</span><br><span class="line">paddle.save(model.state_dict(), save_param_path)</span><br><span class="line">tokenizer.save_pretrained(save_dir)</span><br></pre></td></tr></table></figure><p>训练结果：<br><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/19.png" alt="pic-19"></p><h4 id="模型预测">模型预测</h4><p>加载测试数据集</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">! head -n3 &quot;$&#123;HOME&#125;/.paddlenlp/datasets/LCQMC/lcqmc/lcqmc/test.tsv&quot;</span><br></pre></td></tr></table></figure><p>定义预测函数，转换函数和预测模型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">def predict(model, data_loader):</span><br><span class="line">    batch_probs = []</span><br><span class="line">    # 预测阶段打开 eval 模式，模型中的 dropout 等操作会关掉</span><br><span class="line">    model.eval()</span><br><span class="line">    with paddle.no_grad():</span><br><span class="line">        for batch_data in data_loader:</span><br><span class="line">            input_ids, token_type_ids = batch_data</span><br><span class="line">            input_ids = paddle.to_tensor(input_ids)</span><br><span class="line">            token_type_ids = paddle.to_tensor(token_type_ids)</span><br><span class="line">            # 获取每个样本的预测概率: [batch_size, 2] 的矩阵</span><br><span class="line">            batch_prob = model(</span><br><span class="line">                input_ids=input_ids, token_type_ids=token_type_ids).numpy()</span><br><span class="line"></span><br><span class="line">            batch_probs.append(batch_prob)</span><br><span class="line">        batch_probs = np.concatenate(batch_probs, axis=0)</span><br><span class="line"></span><br><span class="line">        return batch_probs</span><br><span class="line">      </span><br><span class="line"># 预测数据的转换函数</span><br><span class="line"># predict 数据没有 label, 因此 convert_exmaple 的 is_test 参数设为 True</span><br><span class="line">trans_func = partial(</span><br><span class="line">    convert_example,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    max_seq_length=512,</span><br><span class="line">    is_test=True)</span><br><span class="line"></span><br><span class="line"># 预测数据的组 batch 操作</span><br><span class="line"># predict 数据只返回 input_ids 和 token_type_ids，因此只需要 2 个 Pad 对象作为 batchify_fn</span><br><span class="line">batchify_fn = lambda samples, fn=Tuple(</span><br><span class="line">    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids</span><br><span class="line">    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment_ids</span><br><span class="line">): [data for data in fn(samples)]</span><br><span class="line"></span><br><span class="line"># 加载预测数据</span><br><span class="line">test_ds = load_dataset(&quot;lcqmc&quot;, splits=[&quot;test&quot;])</span><br><span class="line"></span><br><span class="line">batch_sampler = paddle.io.BatchSampler(test_ds, batch_size=32, shuffle=False)</span><br><span class="line"></span><br><span class="line"># 生成预测数据 data_loader</span><br><span class="line">predict_data_loader =paddle.io.DataLoader(</span><br><span class="line">        dataset=test_ds.map(trans_func),</span><br><span class="line">        batch_sampler=batch_sampler,</span><br><span class="line">        collate_fn=batchify_fn,</span><br><span class="line">        return_list=True)</span><br><span class="line">        </span><br><span class="line"># 定义预测模型</span><br><span class="line">pretrained_model = paddlenlp.transformers.ErnieGramModel.from_pretrained(&#x27;ernie-gram-zh&#x27;)</span><br><span class="line">model = PointwiseMatching(pretrained_model)</span><br><span class="line"></span><br><span class="line">#加载模型数据</span><br><span class="line">state_dict = paddle.load(&quot;./ernie_gram_zh_pointwise_matching_model/model_state.pdparams&quot;)</span><br><span class="line">model.set_dict(state_dict)</span><br></pre></td></tr></table></figure><p>开始预测</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for idx, batch in enumerate(predict_data_loader):</span><br><span class="line">    if idx &lt; 1:</span><br><span class="line">        print(batch)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 执行预测函数</span><br><span class="line">y_probs = predict(model, predict_data_loader)</span><br><span class="line"></span><br><span class="line"># 根据预测概率获取预测 label</span><br><span class="line">y_preds = np.argmax(y_probs, axis=1)</span><br><span class="line"></span><br><span class="line">#输出预测结果</span><br><span class="line">test_ds = load_dataset(&quot;lcqmc&quot;, splits=[&quot;test&quot;])</span><br><span class="line"></span><br><span class="line">with open(&quot;lcqmc.tsv&quot;, &#x27;w&#x27;, encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">    f.write(&quot;index\tprediction\n&quot;)    </span><br><span class="line">    for idx, y_pred in enumerate(y_preds):</span><br><span class="line">        f.write(&quot;&#123;&#125;\t&#123;&#125;\n&quot;.format(idx, y_pred))</span><br><span class="line">        text_pair = test_ds[idx]</span><br><span class="line">        text_pair[&quot;label&quot;] = y_pred</span><br><span class="line">        print(text_pair)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://www.ryanieeee.cc/img/paddlenlp-learning/20.png" alt="pic-20"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>循环神经网络学习</title>
      <link href="/2023/08/05/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/08/05/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1>循环神经网络学习</h1><h2 id="一-序列模型">一.序列模型</h2><h3 id="1-为什么要使用序列模型">1. 为什么要使用序列模型</h3><p>在我们之前的卷积神经网络学习中，每一个样本数据都是独立而毫无关联的，我们要做的就是让计算机设计一个函数，让它能够正确匹配图像和标签，但是现实生活中我们所接触到的大部分数据都是“子承父业”式的，比如股市的波动，发烧患者的体温等等，t时刻对应的数据都和前面时刻的数据多多少少发生了关联，那么在这种情况下，我们需要新的，能够处理这种数据的模型，于是序列模型就出现了。如果说卷积神经网络能有效地处理空间信息，那么循环神经网络就是就可以很好的处理与时间相关的序列信息。</p><h3 id="2-自回归模型">2.自回归模型</h3><p><img src="https://www.ryanieeee.cc/img/RNN-learning/1.png" alt="pic-1"><br>解决此类的模型的核心问题有两个：（1）如何算出f（x1,…,xt-1）（2）如何算出p<br>那么下面我们给出两种解决方案：</p><h4 id="2-1-马尔可夫假设">2.1 马尔可夫假设</h4><p>我们假设当前的数据只跟前面tau个数据有关，<img src="https://www.ryanieeee.cc/img/RNN-learning/2.PNG" alt="pic-2"><br>也就是说，我们把(X1,…,Xt-1)等价成了(Xt-tau,…Xt-1)</p><h4 id="2-2-隐变量自回归模型">2.2 隐变量自回归模型</h4><p><img src="https://www.ryanieeee.cc/img/RNN-learning/3.png" alt="pic-3"><br>也就是说，我们保留一些对过去观测的总结值ht，并且同时更新预测值和总结值</p><h2 id="二-文本预处理">二.文本预处理</h2><p>一个常见的文本预处理通常包括：将文本作为字符串加载到内存中；将字符串拆分为词元；建立一个词表，将拆分的词元映射到数字索引；将文本转换为数字索引序列，以方便模型操作。那么下面我将介绍这些步骤的具体操作：</p><h3 id="1-读取数据集">1.读取数据集</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def read_text_file(file_path):</span><br><span class="line">   with open(file_path, &#x27;r&#x27;) as f:</span><br><span class="line">       lines = f.readlines()</span><br><span class="line">   return [re.sub(&#x27;[^A-Za-z]+&#x27;, &#x27; &#x27;, line).strip().lower() for line in lines]</span><br><span class="line"># 我们忽略了标点符号和字母的大小写</span><br></pre></td></tr></table></figure><p>假设我们本地有一个名为data.txt的数据集，我们可以调用read_text_file函数来使用它</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file_path = &#x27;path/to/data.txt&#x27;</span><br><span class="line">lines = read_text_file(file_path)</span><br><span class="line">print(f&#x27;文本总行数: &#123;len(lines)&#125;&#x27;)</span><br></pre></td></tr></table></figure><h3 id="2-词元化">2.词元化</h3><p>下面的tokenize函数将文本行列表（lines）作为输入，<strong>列表中的每个元素是一个文本序列（如一条文本行）</strong>。每个文本序列又被拆分成一个词元列表，<strong>词元（token）是文本的基本单位</strong>。最后，<strong>返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）</strong>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def tokenize(lines, token=&#x27;word&#x27;):  #@save</span><br><span class="line">    &quot;&quot;&quot;将文本行拆分为单词或字符词元&quot;&quot;&quot;</span><br><span class="line">    if token == &#x27;word&#x27;:</span><br><span class="line">        return [line.split() for line in lines]</span><br><span class="line">    elif token == &#x27;char&#x27;:</span><br><span class="line">        return [list(line) for line in lines]</span><br><span class="line">    else:</span><br><span class="line">        print(&#x27;错误：未知词元类型：&#x27; + token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line">for i in range(11):</span><br><span class="line">    print(tokens[i])</span><br></pre></td></tr></table></figure><h3 id="3-词表">3. 词表</h3><p>词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。现在，让我们构建一个字典，通常也叫做词表（vocabulary）， 用来将字符串类型的词元映射到从0开始的数字索引中。 我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为语料（corpus）。 然后根据每个唯一词元的出现频率，为其分配一个数字索引。 很少出现的词元通常被移除，这可以降低复杂性。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">class Vocab:  #@save</span><br><span class="line">    &quot;&quot;&quot;文本词表&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):</span><br><span class="line">        if tokens is None:</span><br><span class="line">            tokens = []</span><br><span class="line">        if reserved_tokens is None:</span><br><span class="line">            reserved_tokens = []</span><br><span class="line">        # 按出现频率排序</span><br><span class="line">        counter = count_corpus(tokens)</span><br><span class="line">        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],</span><br><span class="line">                                   reverse=True)</span><br><span class="line">        # 未知词元的索引为0</span><br><span class="line">        self.idx_to_token = [&#x27;&lt;unk&gt;&#x27;] + reserved_tokens</span><br><span class="line">        self.token_to_idx = &#123;token: idx</span><br><span class="line">                             for idx, token in enumerate(self.idx_to_token)&#125;</span><br><span class="line">        for token, freq in self._token_freqs:</span><br><span class="line">            if freq &lt; min_freq:</span><br><span class="line">                break</span><br><span class="line">            if token not in self.token_to_idx:</span><br><span class="line">                self.idx_to_token.append(token)</span><br><span class="line">                self.token_to_idx[token] = len(self.idx_to_token) - 1</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, tokens):</span><br><span class="line">        if not isinstance(tokens, (list, tuple)):</span><br><span class="line">            return self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        return [self.__getitem__(token) for token in tokens]</span><br><span class="line"></span><br><span class="line">    def to_tokens(self, indices):</span><br><span class="line">        if not isinstance(indices, (list, tuple)):</span><br><span class="line">            return self.idx_to_token[indices]</span><br><span class="line">        return [self.idx_to_token[index] for index in indices]</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def unk(self):  # 未知词元的索引为0</span><br><span class="line">        return 0</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def token_freqs(self):</span><br><span class="line">        return self._token_freqs</span><br><span class="line"></span><br><span class="line">def count_corpus(tokens):  #@save</span><br><span class="line">    &quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;</span><br><span class="line">    # 这里的tokens是1D列表或2D列表</span><br><span class="line">    if len(tokens) == 0 or isinstance(tokens[0], list):</span><br><span class="line">        # 将词元列表展平成一个列表</span><br><span class="line">        tokens = [token for line in tokens for token in line]</span><br><span class="line">    return collections.Counter(tokens)</span><br></pre></td></tr></table></figure><ul class="lvl-0"><li class="lvl-2"><p>tokens：可选参数，输入的词元列表。</p></li><li class="lvl-2"><p>min_freq：可选参数，最小词频阈值。低于该阈值的词将被忽略。</p></li><li class="lvl-2"><p>reserved_tokens：可选参数，保留的特殊词元列表。</p></li><li class="lvl-2"><p>_token_freqs：私有变量，词元及其对应的频率排序后的列表。</p></li><li class="lvl-2"><p>idx_to_token：索引到词元的映射的列表。未知词元的索引为0。</p></li><li class="lvl-2"><p>token_to_idx：词元到索引的映射的字典。</p></li><li class="lvl-2"><p><strong>len</strong>(self): 返回词表中词元的数量。</p></li><li class="lvl-2"><p><strong>getitem</strong>(self, tokens): 根据给定的词元或词元列表返回对应的索引或索引列表。</p></li><li class="lvl-2"><p>to_tokens(self, indices): 根据给定的索引或索引列表返回对应的词元或词元列表。</p></li><li class="lvl-2"><p>unk: 未知词元的索引。</p></li><li class="lvl-2"><p>token_freqs: 返回词元及其频率的列表。</p></li><li class="lvl-2"><p>count_corpus(tokens)是一个辅助函数，用于统计给定语料库中的词元频率。</p></li><li class="lvl-2"><p>tokens：词元列表，可以是一维或二维列表。下面我将引用别人的一个例子，展示将文本行转换成数字索引表</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for i in [0, 10]:</span><br><span class="line">    print(&#x27;文本:&#x27;, tokens[i])</span><br><span class="line">    print(&#x27;索引:&#x27;, vocab[tokens[i]])</span><br></pre></td></tr></table></figure><p>结果如下文本: [‘the’, ‘time’, ‘machine’, ‘by’, ‘h’, ‘g’, ‘wells’]<br>索引: [1, 19, 50, 40, 2183, 2184, 400]<br>文本: [‘twinkled’, ‘and’, ‘his’, ‘usually’, ‘pale’, ‘face’, ‘was’, ‘flushed’, ‘and’, ‘animated’, ‘the’]<br>索引: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]</p><h3 id="4-整合功能">4.整合功能</h3><p>在使用上述函数时，我们将所有功能打包到load_corpus_time_machine函数中， 该函数返回corpus（词元索引列表）和vocab（词表）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def load_corpus_txt_file(max_tokens=-1):  #@save</span><br><span class="line">    &quot;&quot;&quot;返回数据集的词元索引列表和词表&quot;&quot;&quot;</span><br><span class="line">    lines = read_text_file()</span><br><span class="line">    tokens = tokenize(lines, &#x27;char&#x27;)</span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    # 因为数据集中的每个文本行不一定是一个句子或一个段落，</span><br><span class="line">    # 所以将所有文本行展平到一个列表中</span><br><span class="line">    corpus = [vocab[token] for line in tokens for token in line]</span><br><span class="line">    if max_tokens &gt; 0:</span><br><span class="line">        corpus = corpus[:max_tokens]</span><br><span class="line">    return corpus, vocab</span><br><span class="line"></span><br><span class="line">corpus, vocab = load_corpus_txt_file()</span><br><span class="line">len(corpus), len(vocab)</span><br></pre></td></tr></table></figure><h2 id="三-循环神经网络">三.循环神经网络</h2><h3 id="1-RNN网络结构">1. RNN网络结构</h3><p>在rnn中我们放弃使用n元语言模型，把自回归模型改进如下图所示结构：<img src="https://www.ryanieeee.cc/img/RNN-learning/4.png" alt="pic-4"><br>我们可以看到，在循环神经网络中，我们使用了Ht来储存前面时刻的状态。在任意时间步t，隐状态的计算可以被视为：<br>1.拼接当前时间步t的输入Xt和前一时间步t−1的隐状态Ht-1<br>2.将拼接的结果送入带有激活函数$的全连接层，全连接层的输出是当前时间步t的隐状态Ht</p><h3 id="2-困惑度——评估语言模型的质量">2. 困惑度——评估语言模型的质量</h3><p><img src="https://www.ryanieeee.cc/img/RNN-learning/5.png" alt="pic-5"><br>其实就是平均交叉熵作指数当困惑度为1时，是最完美的状态，当困惑度为无穷大时，这个模型就没啥用了hhhh</p><h3 id="3-代码实现">3.代码实现</h3><p>网络代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">class RNNModel(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, rnn_layer, vocab_size, **kwargs):</span><br><span class="line">        super(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer  #rnn层</span><br><span class="line">        self.vocab_size = vocab_size  #词表大小，表示模型训练文本的不重复的词汇数量</span><br><span class="line">        self.num_hiddens = self.rnn.hidden_size #获取RNN隐藏状态的大小，并将其赋值给模型的num_hiddens属性</span><br><span class="line">        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1</span><br><span class="line">        if not self.rnn.bidirectional:</span><br><span class="line">            self.num_directions = 1</span><br><span class="line">            #定义一个全连接层，将隐藏状态的大小映射到词表的大小</span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class="line">        else:</span><br><span class="line">            self.num_directions = 2</span><br><span class="line">            #定义一个全连接层，将双向隐藏状态的大小映射到词表的大小</span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)</span><br><span class="line"></span><br><span class="line">    def forward(self, inputs, state):</span><br><span class="line">        #将输入的序列进行one-hot编码，转换为形状为(时间步数, 批量大小, 词表大小)的张量</span><br><span class="line">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class="line">        X = X.to(torch.float32)</span><br><span class="line">        Y, state = self.rnn(X, state) #通过RNN层处理输入序列和初始状态，返回输出Y和更新后的状态state</span><br><span class="line">        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span><br><span class="line">        # 它的输出形状是(时间步数*批量大小,词表大小)。</span><br><span class="line">        output = self.linear(Y.reshape((-1, Y.shape[-1])))#将输出Y进行形状转换，并通过全连接层linear得到最终的预测结果output。</span><br><span class="line">                                                            #返回output和最新的状态state</span><br><span class="line">        return output, state</span><br><span class="line"></span><br><span class="line">    def begin_state(self, device, batch_size=1):  #定义了获取初始状态的方法</span><br><span class="line">        if not isinstance(self.rnn, nn.LSTM):</span><br><span class="line">            # nn.GRU以张量作为隐状态</span><br><span class="line">            return  torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class="line">                                 batch_size, self.num_hiddens),</span><br><span class="line">                                device=device)</span><br><span class="line">        else:</span><br><span class="line">            # nn.LSTM以元组作为隐状态</span><br><span class="line">            return (torch.zeros((</span><br><span class="line">                self.num_directions * self.rnn.num_layers,</span><br><span class="line">                batch_size, self.num_hiddens), device=device),</span><br><span class="line">                    torch.zeros((</span><br><span class="line">                        self.num_directions * self.rnn.num_layers,</span><br><span class="line">                        batch_size, self.num_hiddens), device=device))</span><br><span class="line">  &#x27;&#x27;&#x27;</span><br><span class="line">  如果RNN层不是nn.LSTM，则返回一个全零张量作为初始状态，形状为(方向数 * 层数, 批量大小, 隐藏单元数)。</span><br><span class="line">  如果是nn.LSTM，则返回一个元组，包含两个全零张量，分别表示初始的隐藏状态和cell状态。</span><br><span class="line">  &#x27;&#x27;&#x27;</span><br></pre></td></tr></table></figure><p>定义辅助函数:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">#定义数据集加载函数</span><br><span class="line">def load_data_txt_file(batch_size, num_steps,</span><br><span class="line">                           use_random_iter=False, max_tokens=10000):</span><br><span class="line">  #返回数据集的迭代器和词汇表.</span><br><span class="line"></span><br><span class="line">    data_iter = SeqDataLoader(</span><br><span class="line">        batch_size, num_steps, use_random_iter, max_tokens)</span><br><span class="line">    return data_iter, data_iter.vocab</span><br><span class="line"> </span><br><span class="line"> #定义显示动画函数</span><br><span class="line"> def use_svg_display():</span><br><span class="line">    #Use the svg format to display a plot in Jupyter.</span><br><span class="line">    backend_inline.set_matplotlib_formats(&#x27;svg&#x27;)</span><br><span class="line"> </span><br><span class="line"> class Animator:  #@save</span><br><span class="line">    &quot;&quot;&quot;在动画中绘制数据&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,</span><br><span class="line">                 ylim=None, xscale=&#x27;linear&#x27;, yscale=&#x27;linear&#x27;,</span><br><span class="line">                 fmts=(&#x27;-&#x27;, &#x27;m--&#x27;, &#x27;g-.&#x27;, &#x27;r:&#x27;), nrows=1, ncols=1,</span><br><span class="line">                 figsize=(3.5, 2.5)):</span><br><span class="line">        # 增量地绘制多条线</span><br><span class="line">        if legend is None:</span><br><span class="line">            legend = []</span><br><span class="line">        use_svg_display()</span><br><span class="line">        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line">        if nrows * ncols == 1:</span><br><span class="line">            self.axes = [self.axes, ]</span><br><span class="line">        # 使用lambda函数捕获参数</span><br><span class="line">        self.config_axes = lambda: d2l.set_axes(</span><br><span class="line">            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class="line">        self.X, self.Y, self.fmts = None, None, fmts</span><br><span class="line"></span><br><span class="line">    def add(self, x, y):</span><br><span class="line">        # 向图表中添加多个数据点</span><br><span class="line">        if not hasattr(y, &quot;__len__&quot;):</span><br><span class="line">            y = [y]</span><br><span class="line">        n = len(y)</span><br><span class="line">        if not hasattr(x, &quot;__len__&quot;):</span><br><span class="line">            x = [x] * n</span><br><span class="line">        if not self.X:</span><br><span class="line">            self.X = [[] for _ in range(n)]</span><br><span class="line">        if not self.Y:</span><br><span class="line">            self.Y = [[] for _ in range(n)]</span><br><span class="line">        for i, (a, b) in enumerate(zip(x, y)):</span><br><span class="line">            if a is not None and b is not None:</span><br><span class="line">                self.X[i].append(a)</span><br><span class="line">                self.Y[i].append(b)</span><br><span class="line">        self.axes[0].cla()</span><br><span class="line">        for x, y, fmt in zip(self.X, self.Y, self.fmts):</span><br><span class="line">            self.axes[0].plot(x, y, fmt)</span><br><span class="line">        self.config_axes()</span><br><span class="line">        display.display(self.fig)</span><br><span class="line">        display.clear_output(wait=True)</span><br><span class="line"> </span><br><span class="line"> #定义训练函数</span><br><span class="line">def train(net, train_iter, vocab, lr, num_epochs, device,</span><br><span class="line">              use_random_iter=False):</span><br><span class="line">    </span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=&#x27;epoch&#x27;, ylabel=&#x27;perplexity&#x27;,</span><br><span class="line">                            legend=[&#x27;train&#x27;], xlim=[10, num_epochs])</span><br><span class="line">    # Initialize</span><br><span class="line">    if isinstance(net, nn.Module):</span><br><span class="line">        updater = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    else:</span><br><span class="line">        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)</span><br><span class="line">    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)</span><br><span class="line">    # Train and predict</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        ppl, speed = train_epoch_ch8(</span><br><span class="line">            net, train_iter, loss, updater, device, use_random_iter)</span><br><span class="line">        if (epoch + 1) % 10 == 0:</span><br><span class="line">            print(predict(&#x27;time traveller&#x27;))</span><br><span class="line">            animator.add(epoch + 1, [ppl])</span><br><span class="line">    print(f&#x27;perplexity &#123;ppl:.1f&#125;, &#123;speed:.1f&#125; tokens/sec on &#123;str(device)&#125;&#x27;)</span><br><span class="line">    print(predict(&#x27;time traveller&#x27;))</span><br><span class="line">    print(predict(&#x27;traveller&#x27;))</span><br></pre></td></tr></table></figure><p>初始化模型：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_steps = 32, 35</span><br><span class="line">train_iter, vocab = load_data_txt_file(batch_size, num_steps)</span><br><span class="line">num_hiddens = 256</span><br><span class="line">rnn_layer = nn.RNN(len(vocab), num_hiddens)</span><br><span class="line">state = torch.zeros((1, batch_size, num_hiddens))</span><br><span class="line">X = torch.rand(size=(num_steps, batch_size, len(vocab)))</span><br><span class="line">Y, state_new = rnn_layer(X, state)</span><br></pre></td></tr></table></figure><p>开始训练：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line">net = RNNModel(rnn_layer, vocab_size=len(vocab))</span><br><span class="line">net = net.to(device)</span><br><span class="line">num_epochs, lr = 500, 1</span><br><span class="line">train(net, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p>结果如下：<img src="https://www.ryanieeee.cc/img/RNN-learning/6.png" alt="pic-6"><br>虽然曲线蛮好的，但实际上这个输出的句子还是没法读hhh</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络学习</title>
      <link href="/2023/08/05/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/08/05/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1>卷积神经网络学习</h1><h2 id="一-为什么要卷积">一.为什么要卷积</h2><p>也许会问，为啥我们不直接搞一个全连接网络，而是使用卷积操作呢？答案其实很简单：全连接层结构太复杂了，计算量太大了，换而言之就是性价比太低。<img src="https://www.ryanieeee.cc/img/CNN-learning/1.png" alt="pic-1"><br>比如一张1000x1000的图片，假设有1M个隐藏层，如果我们像左侧一样搞一个全连接层的话，就意味着每一个隐藏层的神经元都要和图片的每一个像素点一一对应，也就是说图片的每一个像素点都要挨个过一遍1M个隐藏层，意味着需要有10^12个连接和权值，这样来看不太现实。<br>但是假如我们加上卷积操作呢？我认为所谓卷积其实就是一个过滤器，用来提取特征减少计算量的，那么假如我们在全连接层前面加上一个10x10的卷积，那么最后实际上只需要10<em>10</em>1000*1000=10^8个连接，相较于之前已经少了很多。<br>那么进一步想，如果我在全连接层前面加上更多的不同的卷积操作，那么实际上传播到全连接层时，我的计算量就少了很多了。</p><table><tr><td bgcolor=Yellow>总结一下：卷积就是一个个过滤器，能过滤出图片的特征，减少计算量。</td></tr></table><h2 id="二-卷积神经网络的结构">二.卷积神经网络的结构</h2><p><img src="https://www.ryanieeee.cc/img/CNN-learning/2.PNG" alt="pic-2"><br>从图中可以看到，我们实现的功能是一个图片分类功能，输入一张图片，计算机需要识别出属于什么交通工具。可以看到，从第一层到最后一层，图片是越来越“<strong>让人看不懂的</strong>”，这个过程就是提取特征的过程，我们把一个汽车图片，经过多种层的处理，最后提取到适合计算机寻找规律的特征。那么下面，我将说一下一个最基本的CNN网络结构究竟都含有哪些层。</p><h3 id="1-输出层——input">1. 输出层——input</h3><p>虽然上面的图片并没有显示出来输入层，但其实每一个神经网络的最底层都是一个输入层，那么输入层是干啥的呢？输入层主要是对数据进行一些处理，比如标准化/归一化处理等，这样做的好处是规范数据，减少异常值干扰，提高模型的训练速度。</p><h3 id="2-卷积层——conv">2. 卷积层——conv</h3><p>我们使用卷积层来提取局部特征。那局部特征到底是咋样提取出来的呢？假设我们现在有一个这样的3x3卷积核：<br>[(1,0,1),<br>(0,1,0),<br>(1,0,1)]<br>我们可以如下图一样进行卷积计算：<img src="https://www.ryanieeee.cc/img/CNN-learning/3.gif" alt="pic-3"><br>显而易见，其实就是两个矩阵作内积。最后输出的矩阵比原来小，这也是我们通过卷积能减少计算量的直观体现。</p><h4 id="权值共享">权值共享</h4><p>在这里我还想介绍一下一个非常重要的机制——权值共享。<img src="https://www.ryanieeee.cc/img/CNN-learning/4.PNG" alt="pic-4"><br>有权值共享机制的卷积运算，我们不难看出，权值共享机制的存在使得输出图像的每一部分都是由同一个卷积核与输入图像运算而得，而如果没有权值共享机制，就会发生以下现象：<img src="https://www.ryanieeee.cc/img/CNN-learning/5.png" alt="pic-5"><br>假设输入图像是192x192，那么有权值共享的卷积核大小只有3<em>3</em>k(k为通道数)，而没有权值共享的卷积核大小为192<em>192</em>k，相较于之前，大了4096倍，由此可以看出，<strong>权值共享机制减少中间参数从而减少了计算量</strong></p><h3 id="3-池化层-下采样-——pool">3. 池化层(下采样)——pool</h3><p>池化层的作用是压缩输入的特征图，一方面简化网络计算复杂度，另一方面进一步提取特征。池化层的两个特点：特征不变性，特征降维<img src="https://www.ryanieeee.cc/img/CNN-learning/6.png" alt="pic-6"><br>上图所示的是最常用的最大池化操作，除此以外还有平均和重叠。池化层还有一个重要的作用就是：<strong>减少过拟合</strong></p><h3 id="4-激活函数（激励层）">4. 激活函数（激励层）</h3><h4 id="1-为啥要用激活函数">1. 为啥要用激活函数</h4><p>提高模型的表达能力，具体一点就是说给模型加上非线性因素，这样模型就可以不仅拟合线性也可以拟合非线性，从而增加了模型的表达能力</p><h4 id="2-常用的激活函数">2. 常用的激活函数</h4><p><img src="https://www.ryanieeee.cc/img/CNN-learning/7.png" alt="pic-7"></p><h3 id="5-全连接层——FC">5. 全连接层——FC</h3><p>在整个神经网络中起到“分类器”的作用，如果说卷积层，池化层，激活函数等操作是将原始数据映射到隐层特征空间的话，那么全连接层就是将前面采集到的“分布式特征表示”映射到样本空间。<br><img src="https://www.ryanieeee.cc/img/CNN-learning/8.png" alt="pic-8">怎么讲呢，其实全连接层就相当于这个蚂蚁大会了。</p><h2 id="三-数据训练">三.数据训练</h2><h3 id="1-前向训练">1.前向训练</h3><p>前向传播就是数据从输入层开始，经过一层一层的处理，最后数据传到输出层的这个过程，最后输出值就是预测值</p><h3 id="2-反向传播">2.反向传播</h3><p>那么既然前向传播输出了预测值，那么我就可以计算出预测值与目标值的误差loss。反向传播就是根据损失函数loss来反方向的计算出每一层的偏导数，从最后一层逐层向前改变每一层的权重，也就是更新参数，其核心就是：<br><strong>损失函数对每一层的每一个参数求偏导的链式求导法则</strong></p><h4 id="2-1计算总误差">2.1计算总误差</h4><p>output：前向传播得到的预测值<br>target：样本值<br><img src="https://www.ryanieeee.cc/img/CNN-learning/9.png" alt="pic-9"></p><h4 id="2-2隐藏层与输出层之间的权重更新">2.2隐藏层与输出层之间的权重更新</h4><p>以权重w5为例，对参数w5求偏导可以看出w5对整体误差的影响，总误差对w5求偏导如下：<img src="https://www.ryanieeee.cc/img/CNN-learning/10.png" alt="pic-10"><br>分解如下：<img src="https://www.ryanieeee.cc/img/CNN-learning/11.png" alt="pic-11"><br>最后三者相乘，下面就是进行梯度下降更新参数：<img src="https://www.ryanieeee.cc/img/CNN-learning/12.png" alt="pic-12"><br>其中η为学习率（learning rate），其余权重同理.</p><h4 id="2-3输出层与隐藏层之间的权重更新">2.3输出层与隐藏层之间的权重更新</h4><p>以权重w1为例：<img src="https://www.ryanieeee.cc/img/CNN-learning/13.png" alt="pic-13"><br>分解如下:<br><img src="https://www.ryanieeee.cc/img/CNN-learning/14.png" alt="pic-14"><br>最后三者相乘，梯度下降，更新参数：<br><img src="https://www.ryanieeee.cc/img/CNN-learning/15.png" alt="pic-15"></p><h4 id="2-4总结">2.4总结</h4><p>神经网络就是通过不断的前向传播和反向传播不断调整神经网络的权重，最终到达预设的迭代次数或者对样本的学习已经到了比较好的程度后，就停止迭代，那么一个神经网络就训练好了。这就是神经网络的本质：通过计算误差、不断修正权重以拟合输入输出的映射函数曲线。</p><h2 id="四-LetNet-5">四.LetNet-5</h2><h3 id="1-基本结构">1. 基本结构</h3><p>LetNet是历史上第一个真正的卷积神经网络，其网络结构如下：<br><img src="https://www.ryanieeee.cc/img/CNN-learning/16.png" alt="pic-16"></p><ul class="lvl-0"><li class="lvl-2"><p>C1层是卷积层，有6个通道，是由输入图像经过6个5x5的卷积核对输入图像卷积得到。</p></li><li class="lvl-2"><p>S1层是一个下采样层（池化层），有6个通道，是由C1层的特征图经过2x2，步长为2的窗口进行平均池化，在利用sigmoid激活函数变换得到。</p></li><li class="lvl-2"><p>C3是卷积层，有16个通道，是由16个5x5的卷积核对S2进行卷积得到。</p></li><li class="lvl-2"><p>S4是一个下采样层（池化），有16个通道，是由C3层的特征图经过2x2，步长为2的窗口进行平均池化，在利用sigmoid激活函数变换得到。</p></li><li class="lvl-2"><p>C5是卷积层包含120个特征图，是由120个5x5的卷积核对S2进行卷积得到。</p></li><li class="lvl-2"><p>F6是包含84个神经元的全连接层，采用双曲正切激活函数。</p></li><li class="lvl-2"><p>output是输出层有10个神经元<br>网络流程如下：<br><img src="https://www.ryanieeee.cc/img/CNN-learning/17.png" alt="pic-17"></p></li><li class="lvl-2"><p>k_size：代表卷积核的大小</p></li><li class="lvl-2"><p>k_num：代表卷积的数量</p></li><li class="lvl-2"><p>s：代表步长大小</p></li></ul><h3 id="2-代码实现">2. 代码实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.optim as optim</span><br><span class="line">from torchvision import datasets, transforms</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line"># 定义LeNet网络结构</span><br><span class="line">class LeNet(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(LeNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(1, 6, kernel_size=5,padding=2),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.AvgPool2d(kernel_size=2, stride=2),</span><br><span class="line">            nn.Conv2d(6, 16, kernel_size=5),</span><br><span class="line">            nn.SIgmoid(),</span><br><span class="line">            nn.AvgPool2d(kernel_size=2, stride=2)</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(16 * 4 * 4, 120),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(120, 84),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(84, 10)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = x.view(x.size(0), -1)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"># 加载MNIST数据集</span><br><span class="line">train_dataset = datasets.MNIST(root=&#x27;./data&#x27;, train=True, transform=transforms.ToTensor(), download=True)</span><br><span class="line">test_dataset = datasets.MNIST(root=&#x27;./data&#x27;, train=False, transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"># 定义训练和测试的数据加载器</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)</span><br><span class="line"></span><br><span class="line"># 创建LeNet模型</span><br><span class="line">net = LeNet().to(device)</span><br><span class="line"></span><br><span class="line"># 定义损失函数和优化器</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=0.001)</span><br><span class="line"></span><br><span class="line"># 将模型设置为训练模式</span><br><span class="line">net.train()</span><br><span class="line"></span><br><span class="line"># 记录训练和测试的准确率和损失</span><br><span class="line">train_acc_history = []</span><br><span class="line">train_loss_history = []</span><br><span class="line">test_acc_history = []</span><br><span class="line">test_loss_history = []</span><br><span class="line"></span><br><span class="line"># 训练模型</span><br><span class="line">num_epochs = 5</span><br><span class="line">for epoch in range(num_epochs):</span><br><span class="line">    train_loss = 0.0</span><br><span class="line">    train_total = 0</span><br><span class="line">    train_correct = 0</span><br><span class="line"></span><br><span class="line">    for i, (images, labels) in enumerate(train_loader):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        outputs = net(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        </span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        train_loss += loss.item() * images.size(0)</span><br><span class="line">        _, predicted = torch.max(outputs.data, 1)</span><br><span class="line">        train_total += labels.size(0)</span><br><span class="line">        train_correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">    # 计算训练集的准确率和损失</span><br><span class="line">    train_accuracy = train_correct / train_total</span><br><span class="line">    train_loss /= train_total</span><br><span class="line">    </span><br><span class="line">    test_loss = 0.0</span><br><span class="line">    test_total = 0</span><br><span class="line">    test_correct = 0</span><br><span class="line"></span><br><span class="line">    # 在测试集上测试模型的准确率和损失</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        for images, labels in test_loader:</span><br><span class="line">            outputs = net(images)</span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line">            </span><br><span class="line">            test_loss += loss.item() * images.size(0)</span><br><span class="line">            _, predicted = torch.max(outputs.data, 1)</span><br><span class="line">            test_total += labels.size(0)</span><br><span class="line">            test_correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">    # 计算测试集的准确率和损失</span><br><span class="line">    test_accuracy = test_correct / test_total</span><br><span class="line">    test_loss /= test_total</span><br><span class="line"></span><br><span class="line">    print(&#x27;Epoch [&#123;&#125;/&#123;&#125;], Train Loss: &#123;:.4f&#125;, Train Accuracy: &#123;:.2f&#125;%, Test Loss: &#123;:.4f&#125;, Test Accuracy: &#123;:.2f&#125;%&#x27;</span><br><span class="line">          .format(epoch+1, num_epochs, train_loss, train_accuracy*100, test_loss, test_accuracy*100))</span><br><span class="line">    </span><br><span class="line">    # 保存训练和测试的准确率和损失</span><br><span class="line">    train_acc_history.append(train_accuracy)</span><br><span class="line">    train_loss_history.append(train_loss)</span><br><span class="line">    test_acc_history.append(test_accuracy)</span><br><span class="line">    test_loss_history.append(test_loss)</span><br><span class="line"></span><br><span class="line"># 绘制准确率和损失的曲线图</span><br><span class="line">plt.figure(figsize=(10, 5))</span><br><span class="line">plt.plot(range(num_epochs), train_acc_history, label=&#x27;Train Accuracy&#x27;)</span><br><span class="line">plt.plot(range(num_epochs), test_acc_history, label=&#x27;Test Accuracy&#x27;)</span><br><span class="line">plt.xlabel(&#x27;Epoch&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Accuracy&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(&#x27;Accuracy Curve&#x27;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(10, 5))</span><br><span class="line">plt.plot(range(num_epochs), train_loss_history, label=&#x27;Train Loss&#x27;)</span><br><span class="line">plt.plot(range(num_epochs), test_loss_history, label=&#x27;Test Loss&#x27;)</span><br><span class="line">plt.xlabel(&#x27;Epoch&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Loss&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(&#x27;Loss Curve&#x27;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="五-AlexNet">五. AlexNet</h2><h3 id="1-相较于LetNet的改进">1.相较于LetNet的改进</h3><p>（1）相较于LetNet要深很多：AlexNet由8层组成：5个卷积层，2个全连接隐藏层，1个全连接输出层。（2）AlexNet使用ReLU作为激活函数而不sigmoid<br>（3）使用的池化层是最大池化而不是平均池化（4）提出了丢弃法dropout来减少过拟合</p><h3 id="2-网络结构">2.网络结构</h3><p><img src="https://www.ryanieeee.cc/img/CNN-learning/18.png" alt="pic-18"><br>原先由于gpu算力不够，需要两块gpu同时处理，但是现在gpu更先进了，所以我们现在只使用一个gpu就ok了改进图如下：<img src="https://www.ryanieeee.cc/img/CNN-learning/19.png" alt="pic-19"><br>值得注意的一点：原图输入224 × 224，实际上进行了随机裁剪，实际大小为227 × 227</p><h4 id="2-1-卷积层C1">2.1 卷积层C1</h4><ul class="lvl-0"><li class="lvl-2"><p>C1的基本结构为：卷积–&gt;ReLU–&gt;池化</p></li><li class="lvl-2"><p>卷积：输入227 × 227 × 3，96个11×11×3的卷积核，padding = 1，步长stride = 4，因此其FeatureMap大小为(227-11+1×2+4)/4 = 55，即55×55×96</p></li><li class="lvl-2"><p>激活函数：ReLU</p></li><li class="lvl-2"><p>池化：池化核大小3 × 3，不扩充边缘padding = 0，步长stride = 2，因此其FeatureMap输出大小为(55-3+0×2+2)/2=27, 即C1输出为27×27×96（此处未将输出分到两个GPU中，若按照论文将分成两组，每组为27×27×48）</p></li></ul><h4 id="2-2-卷积层C2">2.2 卷积层C2</h4><ul class="lvl-0"><li class="lvl-2"><p>C2的基本结构为：卷积–&gt;ReLU–&gt;池化</p></li><li class="lvl-2"><p>卷积：输入27×27×96，256个5×5×96的卷积核，扩充边缘padding = 2， 步长stride = 1，因此其FeatureMap大小为(27-5+2×2+1)/1 = 27，即27×27×256</p></li><li class="lvl-2"><p>激活函数：ReLU</p></li><li class="lvl-2"><p>池化：池化核大小3 × 3，不扩充边缘padding = 0，步长stride = 2，因此其FeatureMap输出大小为(27-3+0+2)/2=13, 即C2输出为13×13×256（此处未将输出分到两个GPU中，若按照论文将分成两组，每组为13×13×128）</p></li></ul><h4 id="2-3-卷积层C3">2.3 卷积层C3</h4><ul class="lvl-0"><li class="lvl-2"><p>C3的基本结构为：卷积–&gt;ReLU。注意一点：此层没有进行MaxPooling操作</p></li><li class="lvl-2"><p>卷积：输入13×13×256，384个3×3×256的卷积核， 扩充边缘padding = 1，步长stride = 1，因此其FeatureMap大小为(13-3+1×2+1)/1 = 13，即13×13×384</p></li><li class="lvl-2"><p>激活函数：ReLU，即C3输出为13×13×384（此处未将输出分到两个GPU中，若按照论文将分成两组，每组为13×13×192）</p></li></ul><h4 id="2-4-卷积层C4">2.4 卷积层C4</h4><ul class="lvl-0"><li class="lvl-2"><p>C4的基本结构为：卷积–&gt;ReLU。注意一点：<strong>此层也没有进行MaxPooling操作</strong></p></li><li class="lvl-2"><p>卷积：输入13×13×384，384个3×3×384的卷积核， 扩充边缘padding = 1，步长stride = 1，因此其FeatureMap大小为(13-3+1×2+1)/1 = 13，即13×13×384</p></li><li class="lvl-2"><p>激活函数：ReLU，即C4输出为13×13×384（此处未将输出分到两个GPU中，若按照论文将分成两组，每组为13×13×192）</p></li></ul><h4 id="2-5-卷积层C5">2.5 卷积层C5</h4><ul class="lvl-0"><li class="lvl-2"><p>C5的基本结构为：卷积–&gt;ReLU–&gt;池化</p></li><li class="lvl-2"><p>卷积：输入13×13×384，256个3×3×384的卷积核，扩充边缘padding = 1，步长stride = 1，因此其FeatureMap大小为(13-3+1×2+1)/1 = 13，即13×13×256</p></li><li class="lvl-2"><p>激活函数：ReLU</p></li><li class="lvl-2"><p>池化：池化核大小3 × 3， 扩充边缘padding = 0，步长stride = 2，因此其FeatureMap输出大小为(13-3+0×2+2)/2=6, 即C5输出为6×6×256（此处未将输出分到两个GPU中，若按照论文将分成两组，每组为6×6×128）</p></li></ul><h4 id="2-6-全连接层FC6">2.6 全连接层FC6</h4><ul class="lvl-0"><li class="lvl-2"><p>FC6的基本结构为：全连接–&gt;&gt;ReLU–&gt;Dropout</p></li><li class="lvl-2"><p>全连接：此层的全连接实际上是通过卷积进行的，输入6×6×256，4096个6×6×256的卷积核，扩充边缘padding = 0, 步长stride = 1, 因此其FeatureMap大小为(6-6+0×2+1)/1 = 1，即1×1×4096</p></li><li class="lvl-2"><p>激活函数：ReLU</p></li><li class="lvl-2"><p>Dropout：全连接层中去掉了一些神经节点，达到防止过拟合，FC6输出为1×1×4096</p></li></ul><h4 id="2-7-全连接FC7">2.7 全连接FC7</h4><ul class="lvl-0"><li class="lvl-2"><p>FC7的基本结构为：全连接–&gt;&gt;ReLU–&gt;Dropout</p></li><li class="lvl-2"><p>全连接：此层的全连接，输入1×1×4096</p></li><li class="lvl-2"><p>激活函数：ReLU</p></li><li class="lvl-2"><p>Dropout：全连接层中去掉了一些神经节点，达到防止过拟合，FC7输出为1×1×4096</p></li></ul><h4 id="2-8-全连接层FC8">2.8 全连接层FC8</h4><ul class="lvl-0"><li class="lvl-2"><p>FC8的基本结构为：全连接–&gt;&gt;softmax</p></li><li class="lvl-2"><p>全连接：此层的全连接，输入1×1×4096;</p></li><li class="lvl-2"><p>softmax：softmax为1000，FC8输出为1×1×1000</p></li></ul><h3 id="3-AlexNet的主要贡献">3.AlexNet的主要贡献</h3><h4 id="3-1-ReLU激活函数的引入">3.1 ReLU激活函数的引入</h4><p>采用修正线性单元(ReLU)的深度卷积神经网络训练时间比等价的tanh单元要快几倍。而时间开销是进行模型训练过程中很重要的考量因素之一。同时，ReLU有效防止了过拟合现象的出现。由于ReLU激活函数的高效性与实用性，使得它在深度学习框架中占有重要地位。</p><h4 id="3-2-层叠池化操作">3.2 层叠池化操作</h4><p>以往池化的大小PoolingSize与步长stride一般是相等的，例如：图像大小为256*256，PoolingSize=2×2，stride=2，这样可以使图像或是FeatureMap大小缩小一倍变为128，此时池化过程没有发生层叠。但是AlexNet采用了层叠池化操作，即PoolingSize &gt; stride。这种操作非常像卷积操作，可以使相邻像素间产生信息交互和保留必要的联系。论文中也证明，此操作可以有效防止过拟合的发生。</p><h4 id="3-3-Dropout操作">3.3 Dropout操作</h4><p>Dropout操作会将概率小于0.5的每个隐层神经元的输出设为0，即去掉了一些神经节点，达到防止过拟合。那些“失活的”神经元不再进行前向传播并且不参与反向传播。这个技术减少了复杂的神经元之间的相互影响。在论文中，也验证了此方法的有效性。</p><h4 id="3-4-网络层数的增加">3.4 网络层数的增加</h4><p>与原始的LeNet相比，AlexNet网络结构更深，LeNet为5层，AlexNet为8层。在随后的神经网络发展过程中，AlexNet逐渐让研究人员认识到网络深度对性能的巨大影响。</p><h3 id="4-代码实例">4.代码实例</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">import torch</span><br><span class="line">from torch import nn, optim</span><br><span class="line">import torchvision</span><br><span class="line">device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line"></span><br><span class="line">class AlexNet(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(AlexNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(1, 96, 11, 4，1), # in_channels, out_channels, kernel_size, stride, padding</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(3, 2), # kernel_size, stride</span><br><span class="line">            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span><br><span class="line">            nn.Conv2d(96, 256, 5, 1, 2),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(3, 2),</span><br><span class="line">            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span><br><span class="line">            # 前两个卷积层后不使用池化层来减小输入的高和宽</span><br><span class="line">            nn.Conv2d(256, 384, 3, 1, 1),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(384, 384, 3, 1, 1),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(384, 256, 3, 1, 1),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(3, 2)</span><br><span class="line">        )</span><br><span class="line">         # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(256*5*5, 4096),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(0.5),</span><br><span class="line">            nn.Linear(4096, 4096),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(0.5),</span><br><span class="line">            # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span><br><span class="line">            nn.Linear(4096, 10),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, img):</span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[0], -1))</span><br><span class="line">        return output</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>未完待续~~~</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用PaddlePaddle学习OCR</title>
      <link href="/2023/08/05/OCR%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/08/05/OCR%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1>用PaddlePaddle学习OCR</h1><h2 id="一-PaddlePaddle环境配置">一. PaddlePaddle环境配置</h2><ul class="lvl-0"><li class="lvl-2">创建虚拟环境</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pp python=3.8</span><br></pre></td></tr></table></figure><ul class="lvl-0"><li class="lvl-2">激活并进入</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate pp</span><br></pre></td></tr></table></figure><ul class="lvl-0"><li class="lvl-2">安装paddle</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install paddlepaddle-gpu==2.5.1.post116 -f https://www.paddlepaddle.org.cn/whl/windows/mkl/avx/stable.html</span><br></pre></td></tr></table></figure><ul class="lvl-0"><li class="lvl-2">安装paddleocr</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install &quot;paddleocr&gt;=2.0.1&quot; # 推荐使用2.0.1+版本</span><br></pre></td></tr></table></figure><h2 id="二-基本原理">二.基本原理</h2><h3 id="CRNN学习（论文链接）">CRNN学习（<a href="https://arxiv.org/pdf/1507.05717v1.pdf">论文链接</a>）</h3><h3 id="1-1-简介">1.1 简介</h3><p>CRNN是一种卷积循环网络，用于检测基于图像的序列识别问题，特别是场景文字识别问题。CRNN实现了不定长验证结合CNN和RNN的网络结构，使用双向LSTM循环网络进行时序训练，并在最后引入CTC损失函数来实现端到端的不定长序列识别。</p><h3 id="1-2-网络结构">1.2 网络结构</h3><p><img src="https://www.ryanieeee.cc/img/paddleocr-learning/1.PNG" alt="pic-1"><br>网络结构包括三层：卷积层——从输入图像中提取特征序列循环层——预测从卷积层输入过来的特征序列的标签（真实值）分布转录层——把从循环层获取的标签分布通过去重整合等操作转化成最终的识别结果下面给出CRNN的完整网络图：<br><img src="https://www.ryanieeee.cc/img/paddleocr-learning/2.PNG" alt="pic-2"></p><h3 id="1-3各层解析">1.3各层解析</h3><p><strong>卷积层</strong><br><img src="https://www.ryanieeee.cc/img/paddleocr-learning/3.png" alt="pic-3"><br>该层包含卷积和池化操作，与普通CNN训练不同的是，CRNN在训练之前，需要把图片放缩到同一高度（宽度不变），论文中使用的高度是32<br><strong>循环层</strong><br><img src="https://www.ryanieeee.cc/img/paddleocr-learning/4.png" alt="pic-4"><br>CRNN的循环层由一个双向LSTM循环神经网络构成，预测特征序列中的每一个特征向量的标签分布（真实结果的概率列表），循环层的误差被反向传播，最后会转换成特征序列，再把特征序列反馈到卷积层，这个转换操作由论文中定义的&quot;Map-to-Sequence&quot;自定义网络层完成，作为卷积层和循坏层之间连接的桥梁<br><strong>转录层</strong><br><img src="https://www.ryanieeee.cc/img/paddleocr-learning/5.png" alt="pic-5"><br>转换预测到标签序列，移除重复标签，再移除“blank”空格</p><h2 id="三-paddleocr的应用">三.paddleocr的应用</h2><p>使用python脚本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from paddleocr import PaddleOCR, draw_ocr</span><br><span class="line"># Paddleocr目前支持的多语言语种可以通过修改lang参数进行切换</span><br><span class="line"># 例如`ch`, `en`, `fr`, `german`, `korean`, `japan`</span><br><span class="line">ocr = PaddleOCR(use_angle_cls=True, lang=&quot;ch&quot;)  # need to run only once to download and load model into memory</span><br><span class="line">img_path = &#x27;D:/PaddleOCR-release-2.6/learning/1.png&#x27;</span><br><span class="line">result = ocr.ocr(img_path, cls=True)</span><br><span class="line">for idx in range(len(result)):</span><br><span class="line">    res = result[idx]</span><br><span class="line">    for line in res:</span><br><span class="line">        print(line)</span><br><span class="line"># 显示结果</span><br><span class="line">from PIL import Image</span><br><span class="line">result = result[0]</span><br><span class="line">image = Image.open(img_path).convert(&#x27;RGB&#x27;)</span><br><span class="line">boxes = [line[0] for line in result]</span><br><span class="line">txts = [line[1][0] for line in result]</span><br><span class="line">scores = [line[1][1] for line in result]</span><br><span class="line">im_show = draw_ocr(image, boxes, txts, scores, font_path=&#x27;doc/fonts/simfang.ttf&#x27;)</span><br><span class="line">im_show = Image.fromarray(im_show)</span><br><span class="line">im_show.save(&#x27;result.jpg&#x27;)</span><br></pre></td></tr></table></figure><p><img src="https://www.ryanieeee.cc/img/paddleocr-learning/6.png" alt="pic-6"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OCR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch学习笔记</title>
      <link href="/2023/08/01/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/08/01/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1>浅谈Pytorch</h1><p>本文主要介绍和解释一些pytorch中常用的函数以及其使用方法</p><h2 id="1-Dataset类">1.Dataset类</h2><p>可以创建适应任何模型的数据集接口</p><h3 id="1">(1)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os <span class="comment">#python中关于系统的一个库，可以获取文件夹中所有文件的地址</span></span><br></pre></td></tr></table></figure><h3 id="2">(2)</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class MyDataset (Dataset):</span><br><span class="line">    #初始化类</span><br><span class="line">    def __init__ (self,root_dir,label_dir):</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.label_dir = label_dir</span><br><span class="line">        self.path = os.path.join(self.root_dir,self.label_dir) #形成样本路径</span><br><span class="line">        self.img_path = os.listdir(self.path)  #获取样本列表</span><br><span class="line">    #获取样本对</span><br><span class="line">    def __getitem__ (self,idx):</span><br><span class="line">        img_name = self.img_path[idx]  #获取样本名称</span><br><span class="line">        img_item_path = os.path.join(self.root_dir,self.label_dir,img_name)</span><br><span class="line">        img = Image.open(img_item_path)</span><br><span class="line">        label = self.label_dir</span><br><span class="line">        return img, label</span><br><span class="line">    </span><br><span class="line">    def __len__ (self):</span><br><span class="line">        return len(self.img_path)</span><br></pre></td></tr></table></figure><p>root_dir: 根目录<br>label_dir: 标签的地址<br>os.path.join：把括号里的路径结合在一起<br>os.listdir: 制成列表<br>idx: 样本位置</p><h2 id="2-TensorDataset">2.TensorDataset</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = TensorDataset(data,targets)</span><br><span class="line"><span class="comment">#其内部的数据格式：(data_i,targets_i)</span></span><br></pre></td></tr></table></figure><p>功能：用来对tensor数据打包，等同于zip函数的功能<br>用途：通常用于打包数据和标签，返回打包成元组的dataset<br>要求：送入该函数的两组tensor第一个维度大小必须相等</p><h2 id="3-ListDataset">3.ListDataset</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ListDataset(data)</span><br></pre></td></tr></table></figure><p>功能：将list类型数据处理成Dataset类<br>用途：将list转换为dataset类型，可以再送入TensorDataset等函数进一步处理<br>要求：list中无嵌套</p><h2 id="4-TransformDataset">4.TransformDataset</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TransformDataset(dataset,transform)</span><br></pre></td></tr></table></figure><p>功能：对dataset进行transform操作<br>用途：自定义transform操作，对dataset中数据进一步操作<br>要求：第一个参数必须是dataset类数据</p><h2 id="5-TensorBoard">5.TensorBoard</h2><h3 id="SummaryWriter">SummaryWriter()</h3><p>常用函数1: add_scalar<br>其参数:<br>tag: 类似于图表的标题<br>scalar_value: y轴<br>global_step: x轴<br>dataformats: 图像数据格式，默认为“CHW”，也可以改为“HWC”或“HW”等</p><p>代码实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y=x&quot;</span>,i,i)  <span class="comment">#y=x图像</span></span><br><span class="line">writer.close()  <span class="comment">#写完记得关闭</span></span><br></pre></td></tr></table></figure><p>查看方式： 在终端窗口输入(虚拟环境中):</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard--logdir = 文件绝对路径--port = 端口名</span><br></pre></td></tr></table></figure><p>注意：文件的路径不能含有中文，端口名可以不加</p><p>常用函数2. add_image</p><p>其参数:<br>tag: 类似于标题<br>img_tensor: torch.Tensor或np.array类型的图形数据<br>global_step: 记录这是这是第几个子图 （int）<br>dataformats；图像数据格式，默认为“CHW”，也可以改为“HWC”或“HW”等</p><p>代码实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">image_path = <span class="string">&quot;xxx&quot;</span></span><br><span class="line">image_PIL = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line">image_array = np.array(img_PIL)</span><br><span class="line">writer_add_image(<span class="string">&quot;test&quot;</span>,image_array,<span class="number">1</span>,dataformats=(<span class="string">&#x27;HW&#x27;</span>))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h2 id="6-Transforms">6.Transforms</h2><p>(1) 简述：Transforms是pytorch的图像处理工具包，是torchvision模块下的一个一个类的集合，可以对图像或数据进行格式变换，裁剪，缩放，旋转等<br>(2) 运行机制：采用transforms.Compose()，将一系列的transforms有序组合，实现时按照这些方法依次对图像操作<br>(3) 下面介绍一些常见的操作：</p><ol><li class="lvl-3"><p>随机裁剪：transforms.RandomCrop(size)<br>size: 给定的尺寸</p></li><li class="lvl-3"><p>中心裁剪：transforms.CenterCrop(size)<br>size: 给定的尺寸</p></li><li class="lvl-3"><p>随机长宽比裁剪 transforms.RandomResizedCrop(size, scale, ratio)<br>size: 给定的尺寸<br>scale: 随机裁剪的大小区间，如scale=(0.08, 1.0)，表示随机crop出来的图片会在的0.08倍至1倍之间<br>ratio：随机长宽比设置</p></li><li class="lvl-3"><p>上下左右中心裁剪：transforms.FiveCrop(size)<br>返回5个值</p></li><li class="lvl-3"><p>上下左右中心裁剪后翻转: transforms.TenCrop(size)<br>返回10个值</p></li><li class="lvl-3"><p>依概率p水平翻转: transforms.RandomHorizontalFlip(p=0.5)<br>p: 默认为0.5</p></li><li class="lvl-3"><p>依概率p垂直翻转：transforms.RandomVerticalFlip(p=0.5)<br>p: 默认为0.5</p></li><li class="lvl-3"><p>随机旋转：transforms.RandomRotation(degrees, resample=False, expand=False, center=None)<br>degrees: 若为单个数，如 30，则表示在（-30，+30）之间随机旋转若为sequence，如(30，60)，则表示在30-60度之间随机旋转<br>resample: 重采样方法选择<br>expand: 可选扩展标志。如果为true，则扩展输出，使其足够大，以容纳整个旋转图像。如果为false或省略，则使输出图像大小不变。<br>center: 可选为中心旋转还是左上角旋转,默认中心旋转</p></li><li class="lvl-3"><p>重置图像尺寸：resize：transforms.Resize(size, interpolation)<br>size: 尺寸<br>interpolation: 插值方法</p></li><li class="lvl-4"><p>标准化：transforms.Normalize(mean,std)<br>mean: 均值，一般都取(0.5,0.5,0.5)<br>std：标准差，一般都取(0.5,0.5,0.5)</p></li><li class="lvl-4"><p>转为tensor：transforms.ToTensor</p></li><li class="lvl-4"><p>修改亮度、对比度和饱和度：transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)<br>参数分别为：亮度，对比度，饱和度，色调</p></li><li class="lvl-4"><p>转灰度图：transforms.Grayscale(num_output_channels)<br>num_output_channels: 当为1时，正常的灰度图，当为3时， 3 channel with r == g == b</p></li><li class="lvl-4"><p>transforms.RandomChoice(transforms)<br>从给定的一系列transforms中选一个进行操作</p></li><li class="lvl-4"><p>transforms.RandomApply(transforms, p=0.5)<br>给一个transform加上概率，以一定的概率执行该操作</p></li><li class="lvl-4"><p>transforms.RandomOrder<br>将transforms中的操作顺序随机打乱</p></li></ol><p>(4)代码实例:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((32, 32)),  # 缩放</span><br><span class="line">    transforms.RandomCrop(32, padding=4),  # 随机裁剪</span><br><span class="line">    transforms.ToTensor(),  # 图片转张量，同时归一化0-255 ---》 0-1</span><br><span class="line">    transforms.Normalize(norm_mean, norm_std),  # 标准化均值为0标准差为1</span><br><span class="line">])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Dataloader">Dataloader</h2><h3 id="Dataloader与Dataset的关系">Dataloader与Dataset的关系</h3><p>如果把Dataset比作一副扑克，那么Dataloader就相当于手上抓的牌，我们的手就相当于神经网络的输入接口</p><h3 id="torch-utils-data-DataLoader">torch.utils.data.DataLoader()</h3><p>构建可迭代的数据装载器, 我们在训练的时候，每一个for循环，每一次iteration，就是从DataLoader中获取一个batch_size大小的数据的<br>我们常用的几个参数:<br>dataset: Dataset类， 决定数据从哪读取以及如何读取<br>bathsize: 批大小<br>shuffle: 每个epoch是否乱序<br>drop_last: 当样本数不能被batchsize整除时， 是否舍弃最后一批数据<br>num_workers: 表示线程数，数据集较小一点的可以不用设</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
